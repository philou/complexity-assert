* TODO build an assert for a linear algorithm
** DONE play with the thing, and print first results
** DONE O(x) algorithms : do a simple Ordinary Least Squares
*** DONE disable GC for the moment
*** DONE deal with warmup
**** DONE find a way to know if my new algo is better than the previous one
***** DONE have an arena, it generates validating samples, runs the 2 algos, then compares the distance on validation inputs
****** DONE compute the rmse
****** DONE split the classes to files
***** What parameters do I already have ?
****** distribution of input sizes
****** number of runs
****** GC or not
****** warmup or not
**** DONE dump csvs to know how the error changes with the algos
**** DONE detect whether warmup has a consequence
***** It Does. Particularly on small number of runs.
***** Even in the same process, we can spot a difference
***** Once we know if it is important, let's just use it all the time
***** It might be depending on the algorithm ! the number of time some parts are looped over during the run.
***** [OK] On our linear search, around 60 warmup rounds are required to reduce variability
***** DONE play with different counts of warmup rounds
** TODO Add +4 pomodoro to the task title
** DONE O(1) : identify the flat slope
*** DONE not sure a transformation is possible for a regression, otherwise, just do an average
*** DONE predict is just returning the average
*** DONE print and compare the results
*** DONE try with a constant algorithm to test
** DONE need to rule out very small coefs, could find a quadratic with a small coef !
*** 12 times on 55, found linear better than constant, only finds constant 80% of times
*** to compare RMSE corretly, they shouls be normalized, or using the same data points
*** DONE reuse the same data for training and validation
*** If we had an idea of the max size we would like to use it for, we could check that at that size, coeff is still irrelevant
| algo          | slope | y intercept | at 1000 |
|---------------+-------+-------------+---------|
| random 5      | E-10  | E-6         | +10%    |
| linear search | E-7   | E-7         | *1000   |
*** Compute the error we can expect from our data sample, and use it to determine what is 0 or not
**** Don't know how to do it !
**** One thing is that negative slopes are zero !
*** See how it goes with quadratic
*** Could we use special assertion be_constant(within: constraints)
*** It's a case of overfitting in fact !
**** We can give benefit of the doubt to the simpler model.
**** The more complex model needs to give an RMSE half the one of the simpler model to be selected
**** This works well on practice with Random5 and LinearSearch
** TODO O(x2) : pre-treat with an sqrt before OLS, compare RMSE of both linear and quadratic models to see which one is best
** TODO write the basic rspec integration
*** DONE define the API we want
**** class Algorithm; def generate_args(n) ...; def run(args) ...; end;
**** expect(Algorithm).to be_linear()
**** expect(Algorithm).to be_constant()
**** expect(Algorithm).to be_quadratic()
**** expect(Algorithm).to be_logarithmic()
**** expect(Algorithm).to be_in(N*LN(N))
**** expect(Algorithm, warmups: 30, rounds: 20, sizes: [...]).to be_xxx() or expect(Algorithm).to be_xxx(warmups: 30, rounds: 20, sizes: [...])
*** TODO determine the condition
**** compare with constant
***** simple to do ! not sure it works :
| algorithm | expect constant                                                       | expect linear                         |
|-----------+-----------------------------------------------------------------------+---------------------------------------|
| constant  | regress both, find constant is better or linear with very small slope | will find a very small slope          |
| linear    | should fail                                                           | should pass, but might be quadratic ! |
***** checking constant is checking that is not worse than constant eg : linear or log
***** they can all be coded with transformations to the timings
****** constant : remove the size, keep the timing
****** linear : identity
****** logarithmic : some kind of exponential
****** quadratic : some kind of squar root
*** TODO package a simple rspec lib
*** TODO time the overall execution and make it faster
**** TODO reuse the same datapoint for all models
**** TODO cache the linear regression (it's done twice)
*** TODO factorize / find a way to better generate the sizes
** TODO Robustness against GC : use gc intensive ruby methods, and see how the regression behaves
** TODO O(x?) : do some kind of dichotomy or search to find the most probable model
** TODO O(lnx) : pre-treat with exp()
** TODO O(?lnx) : use exp, then a search for the coefficient (aka polynomial)
** TODO O(xlnx) : there is no well known inverse for that, we can compute it numericaly though
